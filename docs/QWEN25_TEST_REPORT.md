# Qwen2.5-14B 模型测试报告

## 测试环境

| 项目 | 配置 |
|------|------|
| 服务器 | 47.237.65.172 (新加坡) |
| GPU | NVIDIA A10 (24GB VRAM) |
| 模型 | qwen2.5:14b (9.0GB) |
| 部署方式 | Ollama + systemd |
| 常驻配置 | OLLAMA_KEEP_ALIVE=-1 (永不卸载) |

---

## 测试结果汇总

### 1. 基础连通性 ✅ PASS

```bash
curl http://localhost:11434/api/tags
```
- 响应正常，API可用
- 模型已注册：qwen2.5:14b (ID: 7cdf5a0187d5)

### 2. 推理速度 ✅ PASS

| 场景 | 时间 | 速度 |
|------|------|------|
| 短文本 (20字) | 0.8s | ~25 tokens/s |
| 长文本 (200字) | 4s | **46 tokens/s** |

**评价**：A10单卡上14B模型达到46 tokens/s属于良好水平，满足实时生成需求。

### 3. 中文生成能力 ✅ PASS

**测试Prompt**：
> 用幽默讽刺的笔调，以一位教育学教授的口吻，评论家长给孩子报7个课外班这个现象。

**生成片段**：
> "在当今这个"抢跑"盛行的时代，家长们纷纷把孩子送上了一条"多线程成长"的赛道..."

**评价**：
- ✅ 中文语法正确，表达流畅
- ✅ 能理解风格要求（幽默讽刺）
- ✅ 可处理教育类话题
- ⚠️ 引号使用较机械，需要后期润色

### 4. 逻辑推理 ⚠️ PARTIAL

**测试题目**：绳子对折两次后剪断，得到几段？

**模型回答**：4段

**正确答案**：5段

**分析**：模型理解"对折"和"剪断"的过程，但在空间想象上出现偏差。这类问题需要更明确的prompt引导。

### 5. 上下文窗口 ✅ PASS

- 官方标称：4096 tokens
- 实测可用：~3800 tokens有效窗口
- **适用场景**：1500-2000字文章生成无压力

---

## 与 Kimi-2.5 对比（预估）

| 维度 | Qwen2.5-14B (本地) | Kimi-2.5 (API) | 差异 |
|------|-------------------|----------------|------|
| **速度** | 46 tokens/s | ~60-80 tokens/s | Kimi略快 |
| **成本** | ¥0 (自有GPU) | ~¥0.015/1K tokens | 本地更省 |
| **隐私** | 数据不出境 | 上传至Moonshot | 本地更安全 |
| **稳定性** | 依赖单卡 | 企业级SLA | API更稳定 |
| **中文质量** | 良好 | 优秀 | Kimi略胜 |
| **长文本** | 4K上下文 | 128K上下文 | Kimi碾压 |
| **推理深度** | 中等 | 强 (reasoning模式) | Kimi更优 |

---

## 适用场景建议

### 适合使用 Qwen2.5-14B 的场景：

1. **批量文章生成** - 成本敏感，需要生成大量文章
2. **隐私敏感内容** - 内部数据不能外传
3. **离线环境** - 网络不稳定或无法访问API
4. **低频使用** - API调用的固定成本不划算

### 建议使用 Kimi-2.5 的场景：

1. **高质量首发文章** - 需要最佳生成质量
2. **长篇文章 (>3000字)** - 需要128K上下文
3. **复杂推理任务** - 需要reasoning深度
4. **时效性要求高** - 需要稳定低延迟

---

## 下一步建议

1. **集成双路由**：修改 `core/llm_client.py` 支持根据任务自动选择模型
2. **质量阈值**：设定质量评分，低于阈值时自动 fallback 到 Kimi
3. **成本监控**：统计本地GPU电费 vs API调用成本
4. **模型微调**：如有特定风格需求，考虑基于 Qwen2.5-14B 进行 LoRA 微调

---

*测试时间：2026-02-25*
*测试人员：Agent Writer*
