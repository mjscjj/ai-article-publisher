#!/usr/bin/env python3
"""
ã€ Autonomous Researcher Core ã€‘
å®Œå…¨è§£è€¦çš„æ ¸å¿ƒç ”ç©¶å¼•æ“ç±»ã€‚å¯å°†å…¶å¯¼å…¥ä»»ä½•éœ€è¦çš„åŒ…å†…ã€‚
"""
import urllib.request
import urllib.parse
import re
import json

class AutonomousResearcher:
    def __init__(self, llm_callable, max_depth=2, max_urls_per_query=2):
        self.llm = llm_callable
        self.max_depth = max_depth
        self.max_urls_per_query = max_urls_per_query
        self.knowledge_base = [] # çŸ¥è¯†æ± 

    def _generate_queries(self, topic, prior_findings="", depth=1):
        """è®©å¤§æ¨¡å‹æ ¹æ®è¯¾é¢˜ï¼Œæ‹†è§£ä¸ºå…·ä½“çš„æœç´¢å¼•æ“å…³é”®è¯"""
        sys_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æƒ…æŠ¥æœç´¢å¼•æ“ä¸“å®¶ã€‚åªè¿”å›åˆæ³•çš„ JSON æ•°ç»„ï¼Œå¦‚ [\"A\", \"B\"]ï¼Œä¸¥ç¦å›ç­”å¤šä½™å­—ç¬¦æˆ–è§£é‡Šã€‚"
        prompt = f"å½“å‰å‘½é¢˜ï¼šã€{topic}ã€‘\n"
        if prior_findings:
            prompt += f"ç›®å‰çº¿ç´¢ï¼š{prior_findings[:1000]}...\nè¯·æ ¹æ®å·²æœ‰çº¿ç´¢ï¼Œè¡¥è¶³ç›²åŒºï¼Œç”Ÿæˆ 3 ä¸ªæ–°çš„æœç´¢å¼•æ“æ£€ç´¢è¯ã€‚\n"
        else:
            prompt += "è¯·å°†è¯¥å‘½é¢˜æ‹†è§£ä¸º 3 ä¸ªå…·ä½“ç²¾å‡†çš„æœç´¢å¼•æ“è¯æ¡ä»¥å¯»æ‰¾æ•°æ®å’Œæ¡ˆä¾‹ã€‚\n"
        try:
            res = self.llm(prompt, sys_prompt)
            match = re.search(r'\[.*?\]', res, re.S)
            if match: return json.loads(match.group(0))
        except Exception: pass
        return [f"{topic} æ·±å…¥åˆ†æ", f"{topic} æ•°æ® æ¡ˆä¾‹", f"{topic} æœ€æ–°è¿›å±•"]

    def _fetch_search_links(self, query):
        """DuckDuckGo HTML æŠ“å–"""
        links = []
        url = "https://html.duckduckgo.com/html/?q=" + urllib.parse.quote(query)
        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/114.0.0.0 Safari/537.36'})
        
        try:
            html = urllib.request.urlopen(req, timeout=10).read().decode('utf-8')
            # å…¼å®¹ duckduckgo html çš„è§£æ
            matches = re.findall(r'<a class="result__url" href="([^"]+)">', html)
            for m in matches:
                link = m
                if 'duckduckgo.com/l/?uddg=' in link:
                    # å‰¥æ´‹è‘±æ‹¿å‡ºçœŸå®å¤–é“¾
                    link = urllib.parse.unquote(link.split('uddg=')[1].split('&rut=')[0])
                if link.startswith('http') and "duckduckgo" not in link:
                    links.append(link)
                if len(links) >= self.max_urls_per_query: break
        except Exception as e:
            print(f"æœç´¢ {query} å‡ºé”™:", e)
        return list(set(links))

    def _scrape_page(self, url):
        """çº¯å‡€è„±æ°´æŠ“å–ç½‘é¡µæ­£æ–‡"""
        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/114.0.0.0 Safari/537.36'})
        try:
            content = urllib.request.urlopen(req, timeout=8).read().decode('utf-8', errors='ignore')
            # åˆ  script, style, header, footer ç­‰æ±¡æŸ“å™ªéŸ³
            content = re.sub(r'<(script|style|nav|footer|header)[^>]*>.*?</\1>', ' ', content, flags=re.S | re.I)
            text = re.sub(r'<[^>]+>', ' ', content)
            text = re.sub(r'\s+', ' ', text)
            return text.strip()[:6000] # æœ€å¤šæˆªå–å‰ 6000 ä¸ªå­—ç¬¦
        except Exception:
            return ""

    def _reflect(self, topic, prior):
        """è‡ªæˆ‘åæ€å›è·¯ï¼šåˆ¤æ–­æ‰“æä¸Šæ¥çš„æ–‡æœ¬æ˜¯å¦è¶³å¤Ÿæœ‰è¥å…»"""
        if not prior: return False
        try:
            res = self.llm(f"è¯¾é¢˜ï¼š{topic}\nå½“å‰å·²è·ææ–™ï¼š{prior[:3000]}ã€‚\nå¦‚æœææ–™å†…åŒ…å«å®è´¨çš„æ•°æ®ã€æ¡ˆä¾‹ã€å¼ºè§‚ç‚¹æ”¯æ’‘é•¿æ–‡åˆ™è¿”å› ENOUGHï¼Œå¦‚æœåªæ˜¯åºŸè¯/å¤ªçŸ­åˆ™è¿”å› NEED_MORE", "ä¸¥ç¦è¯´åºŸè¯").upper()
            return "ENOUGH" in res
        except: return False

    def _synthesize(self, topic):
        """ç”¨ LLM æ¦¨å– Fact-Pack"""
        if not self.knowledge_base:
            return "æœªèƒ½ä»å¤–ç½‘æ•è·åˆ°é«˜è´¨é‡/å³æ—¶çš„ç½‘é¡µæ­£æ–‡ã€‚è¿™å¯èƒ½æ˜¯ç”±äºç›®æ ‡å…³é”®è¯ç›¸å…³çš„ç½‘é¡µå¼€å¯äº†å¼ºåçˆ¬å¯¼è‡´ã€‚ä½†ç³»ç»Ÿä¿ç•™äº†å‘½é¢˜å’ŒçŸ¥è¯†å†…å»ºã€‚è¯·ç›´æ¥ä½¿ç”¨æ¨¡å‹å†…å»ºè§†é‡å†™ä½œã€‚"
        sys = "ä½ æ˜¯ä¸€åé¡¶çº§çš„éè™šæ„æ·±æŒ–ç¼–è¾‘åŠ©æ‰‹ã€‚"
        joined_knowledge = "\n- ".join(self.knowledge_base)[:20000]
        prompt = f"è¿™æ˜¯ç³»ç»Ÿçˆ¬è™«å¼ºè¡Œå¸¦å›çš„å…³äºã€{topic}ã€‘çš„å¤šæ–‡ç¢ç‰‡æ•°æ®ï¼š\n{joined_knowledge}\n\nè¯·ç²¾ç‚¼å‡º 4-6 æ¡æå…·æ´å¯Ÿçš„æ–°é—»äº‹å®åŒ…ï¼ˆFact-Packï¼‰ï¼Œå¿…é¡»åŒ…å«ä½ å…¶ä¸­çœ‹åˆ°çš„çœŸå®ä¾‹å­ã€æ•°å­—å’Œäº‹ä»¶ã€‚"
        return self.llm(prompt, sys)

    def run(self, topic):
        print(f"\nğŸš€ [Autonomous Researcher V3] å¼•æ“å¯åŠ¨ã€‚å‘½é¢˜ï¼š{topic}")
        print(f"âš™ï¸ æœ€å¤§ä¸‹æ½œå±‚æ•°: {self.max_depth}, æ¯è¯æ¡ç‚¹å‡»å¤–é“¾æ•°: {self.max_urls_per_query}")
        
        for depth_layer in range(1, self.max_depth + 1):
            print(f"\nğŸŒŠ [Depth {depth_layer}/{self.max_depth}] å‘èµ·æ·±ç½‘æ¢ç´¢...")
            prior_kb = " | ".join(self.knowledge_base)[:3000] if self.knowledge_base else ""
            
            # AI æ™ºèƒ½å¤šè§†è§’æ‹†è¯
            queries = self._generate_queries(topic, prior_kb, depth=depth_layer)
            print(f"ğŸ¯ AIåˆ†è£‚è§¦æ‰‹å…³é”®è¯: {queries}")
            
            layer_docs = []
            for q in queries:
                print(f"  ğŸ•·ï¸ å¯åŠ¨åˆ†å¸ƒå¼è§¦æ‰‹ -> é¸­é¸­æœ: '{q}'")
                links = self._fetch_search_links(q)
                for link in links:
                    print(f"    ğŸ“„ æ½œå…¥å¹¶å‰¥ç¦»æ­£æ–‡: {link[:50]}...")
                    content = self._scrape_page(link)
                    # åçˆ¬æ‹¦æˆªäº†å¤§é‡è¯·æ±‚ï¼Œè¿™é‡Œè¦æ±‚è‡³å°‘å–åˆ°ç‚¹æ­£æ–‡å­—ç¬¦
                    if content and len(content) > 200:
                        self.knowledge_base.append(content)
                        layer_docs.append(content)
                        
            print(f"ğŸ“¦ æ­¤å±‚æ‰“æç»“æŸã€‚æˆåŠŸè§£æé«˜è´¨é‡å¤–ç½‘æ·±æ½œé•¿æ–‡: {len(layer_docs)} ç¯‡ã€‚")
            
            # AI è‡ªæˆ‘åæ€æœºåˆ¶
            if depth_layer < self.max_depth:
                current_k_str = " ".join(layer_docs)
                print("ğŸ¤” [Self-Reflection] AIæ­£åœ¨åæ€ï¼šç›®å‰æ‰“æçš„äº‹å®è¶³å¤Ÿå†™é•¿ç¯‡ç ”æŠ¥äº†å—ï¼Ÿ")
                if self._reflect(topic, current_k_str):
                    print("ğŸ’¡ AI åæ€åˆ¤å®šï¼šå·²æ”¶é›†è¶³é‡ç¡¬æ ¸äº‹å®ï¼Œç«‹åˆ»ä¸­æ­¢ä¸‹æ½œæœºåˆ¶ä»¥èŠ‚çœæ—¶é—´/ç®—åŠ›ã€‚")
                    break
                else:
                    print("ğŸ“‰ AI åæ€åˆ¤å®šï¼šå½“å‰çš„ææ–™å¹²è´§å¤ªå°‘æˆ–åŒè´¨åŒ–ï¼Œå³å°†å‘èµ·æ›´æ·±ä¸€å±‚çš„å®šå‘æ‰¾å¯»(ç»§ç»­ä¸‹æ½œ)ï¼")
                    
        print("\n=============================================")
        print("ğŸ­ æ‰“ææ”¶å·¥ã€‚å°†å·¨é‡æ®‹ä½™æ–‡æœ¬æ¨å…¥é«˜æ¸©ç†”ç‚‰ï¼Œæç‚¼ç»ˆæéª¨æ¶...")
        fact_pack = self._synthesize(topic)
        print("âœ… =============== ç»ˆæè°ƒç ”åŒ… Fact-Pack ===============")
        print(fact_pack)
        print("========================================================\n")
        return fact_pack

def test_researcher():
    import sys
    # å¼ºåˆ¶å°† core åŠ å…¥æœå¯»è·¯å¾„ï¼Œä»¥ä¾¿èƒ½ import ä½ çš„å…è´¹å¤§æ¨¡å‹ SDK
    sys.path.append("/root/.openclaw/workspace-writer/ai-article-publisher/core")
    from llm_client import ask_ai
    
    # æ¯”å¦‚æˆ‘ä»¬æƒ³è¦äº†è§£å¤§è¯­è¨€æ¨¡å‹o3çš„æŠ€æœ¯å†…å¹•
    r = AutonomousResearcher(llm_callable=ask_ai, max_depth=2, max_urls_per_query=2)
    # è°ƒç”¨
    res = r.run("OpenAI o3-mini æ¨¡å‹å¯¹æ¯” DeepSeek R1 æ€§èƒ½å·®å¼‚å…·ä½“æ¡ˆä¾‹")

if __name__ == "__main__":
    test_researcher()
