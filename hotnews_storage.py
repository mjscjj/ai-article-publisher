#!/usr/bin/env python3
"""
çƒ­ç‚¹æ•°æ®å­˜å‚¨ç³»ç»Ÿ
ç»“æ„åŒ–å­˜å‚¨çƒ­ç‚¹æ•°æ®ï¼Œæ”¯æŒç´¢å¼•å’ŒæŸ¥è¯¢
"""

import json
import os
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from urllib.request import urlopen, Request
from urllib.parse import quote
import re

# é…ç½®
STORAGE_DIR = "/root/.openclaw/workspace-writer/ai-article-publisher/data/hotnews"
RSSHUB_BASE = "http://localhost:1200"

# æ•°æ®æºé…ç½®
SOURCES = {
    # ===== ç»¼åˆçƒ­ç‚¹ =====
    "weibo": {
        "name": "å¾®åšçƒ­æœ",
        "platform": "å¾®åš",
        "url": "/weibo/search/hot",
        "category": "ç»¼åˆ"
    },
    "zhihu": {
        "name": "çŸ¥ä¹çƒ­æ¦œ",
        "platform": "çŸ¥ä¹",
        "url": "/zhihu/hot",
        "category": "ç»¼åˆ"
    },
    "zhihu_daily": {
        "name": "çŸ¥ä¹æ—¥æŠ¥",
        "platform": "çŸ¥ä¹",
        "url": "/zhihu/daily",
        "category": "ç²¾é€‰"
    },
    
    # ===== ç§‘æŠ€åª’ä½“ =====
    "hackernews": {
        "name": "Hacker News",
        "platform": "Hacker News",
        "url": "/hackernews/best",
        "category": "ç§‘æŠ€"
    },
    "github": {
        "name": "GitHub Trending",
        "platform": "GitHub",
        "url": "/github/trending/daily",
        "category": "ç§‘æŠ€"
    },
    "v2ex": {
        "name": "V2EX",
        "platform": "V2EX",
        "url": "/v2ex/topics/hot",
        "category": "ç§‘æŠ€"
    },
    "sspai": {
        "name": "å°‘æ•°æ´¾",
        "platform": "å°‘æ•°æ´¾",
        "url": "/sspai/index",
        "category": "ç§‘æŠ€"
    },
    "ithome": {
        "name": "ITä¹‹å®¶",
        "platform": "ITä¹‹å®¶",
        "url": "/ithome/ranking/7days",
        "category": "ç§‘æŠ€"
    },
    "juejin": {
        "name": "æ˜é‡‘",
        "platform": "æ˜é‡‘",
        "url": "/juejin/trending/all/monthly",
        "category": "ç§‘æŠ€"
    },
    "36kr": {
        "name": "36æ°ª",
        "platform": "36æ°ª",
        "url": "/36kr/news/latest",
        "category": "ç§‘æŠ€"
    },
    
    # ===== è´¢ç» =====
    "cls": {
        "name": "è´¢è”ç¤¾",
        "platform": "è´¢è”ç¤¾",
        "url": "/cls/telegraph",
        "category": "è´¢ç»"
    },
    "wallstreetcn": {
        "name": "åå°”è¡—è§é—»",
        "platform": "åå°”è¡—è§é—»",
        "url": "/wallstreetcn/news/global",
        "category": "è´¢ç»"
    },
    
    # ===== æ–°é—» =====
    "thepaper": {
        "name": "æ¾æ¹ƒæ–°é—»",
        "platform": "æ¾æ¹ƒæ–°é—»",
        "url": "/thepaper/featured",
        "category": "æ–°é—»"
    },
    
    # ===== å¿ƒç†å­¦ =====
    "douban_psychology": {
        "name": "è±†ç“£å¿ƒç†å­¦",
        "platform": "è±†ç“£",
        "url": "/douban/group/psychology",
        "category": "å¿ƒç†å­¦"
    }
}

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
}


def ensure_dir(path: str):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    os.makedirs(path, exist_ok=True)


def generate_id(title: str, source: str, timestamp: str) -> str:
    """ç”Ÿæˆå”¯ä¸€ID"""
    content = f"{title}_{source}_{timestamp}"
    return hashlib.md5(content.encode()).hexdigest()[:12]


def parse_rss(rss_url: str, source_id: str) -> List[Dict]:
    """è§£æRSSå†…å®¹"""
    try:
        full_url = f"{RSSHUB_BASE}{rss_url}"
        req = Request(full_url, headers=HEADERS)
        with urlopen(req, timeout=20) as resp:
            content = resp.read().decode('utf-8')
        
        items = []
        # è§£æ RSS item
        item_pattern = r'<item>.*?<title>(?:<!\[CDATA\[)?(.*?)(?:\]\]>)?</title>.*?<link>(.*?)</link>.*?(?:<pubDate>(.*?)</pubDate>)?.*?</item>'
        matches = re.findall(item_pattern, content, re.DOTALL)
        
        now = datetime.now()
        timestamp = now.strftime("%Y-%m-%d %H:%M:%S")
        date_str = now.strftime("%Y-%m-%d")
        
        for title, link, pubdate in matches:
            title = title.strip()
            link = link.strip()
            
            if not title or title == source_id:
                continue
            
            item = {
                "id": generate_id(title, source_id, timestamp),
                "title": title,
                "url": link,
                "source_id": source_id,
                "source_name": SOURCES[source_id]["name"],
                "platform": SOURCES[source_id]["platform"],
                "category": SOURCES[source_id]["category"],
                "crawl_time": timestamp,
                "crawl_date": date_str,
                "pub_time": pubdate.strip() if pubdate else None
            }
            items.append(item)
        
        return items
    except Exception as e:
        print(f"[{source_id}] è·å–å¤±è´¥: {str(e)[:50]}")
        return []


def save_to_daily(items: List[Dict], date_str: str = None):
    """ä¿å­˜åˆ°æ—¥æœŸæ–‡ä»¶ï¼ˆæŒ‰æ ‡é¢˜å»é‡ï¼‰"""
    if not date_str:
        date_str = datetime.now().strftime("%Y-%m-%d")
    
    # æŒ‰æ—¥æœŸå­˜å‚¨
    daily_file = os.path.join(STORAGE_DIR, "daily", f"{date_str}.json")
    ensure_dir(os.path.dirname(daily_file))
    
    # è¯»å–å·²æœ‰æ•°æ®
    existing = {}
    if os.path.exists(daily_file):
        with open(daily_file, 'r', encoding='utf-8') as f:
            existing = json.load(f)
    
    # å»ºç«‹æ ‡é¢˜ç´¢å¼•ï¼ˆç”¨äºå»é‡ï¼‰
    title_to_id = {item["title"]: item_id for item_id, item in existing.items()}
    
    # åˆå¹¶æ–°æ•°æ®ï¼ˆæŒ‰æ ‡é¢˜å»é‡ï¼‰
    new_count = 0
    for item in items:
        title = item["title"]
        if title in title_to_id:
            # å·²å­˜åœ¨ï¼Œæ›´æ–° crawl_time ä½†ä¿ç•™åŸæ•°æ®
            existing[title_to_id[title]]["crawl_time"] = item["crawl_time"]
        else:
            # æ–°çƒ­ç‚¹ï¼Œæ·»åŠ 
            existing[item["id"]] = item
            title_to_id[title] = item["id"]
            new_count += 1
    
    # ä¿å­˜
    with open(daily_file, 'w', encoding='utf-8') as f:
        json.dump(existing, f, ensure_ascii=False, indent=2)
    
    return new_count


def save_to_source(items: List[Dict], source_id: str):
    """ä¿å­˜åˆ°æ¥æºç´¢å¼•ï¼ˆæŒ‰æ ‡é¢˜å»é‡ï¼‰"""
    source_file = os.path.join(STORAGE_DIR, "by_source", f"{source_id}.json")
    ensure_dir(os.path.dirname(source_file))
    
    # è¯»å–å·²æœ‰æ•°æ®
    existing = []
    if os.path.exists(source_file):
        with open(source_file, 'r', encoding='utf-8') as f:
            existing = json.load(f)
    
    # å»ºç«‹æ ‡é¢˜ç´¢å¼•
    title_set = {item["title"] for item in existing}
    
    # åˆå¹¶æ–°æ•°æ®ï¼ˆæŒ‰æ ‡é¢˜å»é‡ï¼‰
    for item in items:
        if item["title"] not in title_set:
            existing.append(item)
            title_set.add(item["title"])
    
    # åªä¿ç•™æœ€è¿‘ 500 æ¡
    existing = existing[-500:]
    
    # ä¿å­˜
    with open(source_file, 'w', encoding='utf-8') as f:
        json.dump(existing, f, ensure_ascii=False, indent=2)


def update_index(items: List[Dict]):
    """æ›´æ–°ç´¢å¼•"""
    index_file = os.path.join(STORAGE_DIR, "index.json")
    ensure_dir(STORAGE_DIR)
    
    # è¯»å–ç´¢å¼•
    index = {
        "last_update": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "total_items": 0,
        "sources": {},
        "categories": {},
        "dates": {}
    }
    
    if os.path.exists(index_file):
        with open(index_file, 'r', encoding='utf-8') as f:
            index = json.load(f)
    
    # æ›´æ–°ç»Ÿè®¡
    for item in items:
        source_id = item["source_id"]
        category = item["category"]
        date = item["crawl_date"]
        
        # æ¥æºç»Ÿè®¡
        if source_id not in index["sources"]:
            index["sources"][source_id] = {"name": item["source_name"], "count": 0}
        index["sources"][source_id]["count"] += 1
        
        # åˆ†ç±»ç»Ÿè®¡
        if category not in index["categories"]:
            index["categories"][category] = 0
        index["categories"][category] += 1
        
        # æ—¥æœŸç»Ÿè®¡
        if date not in index["dates"]:
            index["dates"][date] = 0
        index["dates"][date] += 1
    
    index["total_items"] = sum(s["count"] for s in index["sources"].values())
    index["last_update"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # ä¿å­˜
    with open(index_file, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)


def crawl_all_sources(sources: List[str] = None, include_extended: bool = True) -> Dict[str, int]:
    """é‡‡é›†æ‰€æœ‰æ•°æ®æºï¼ˆåŒ…æ‹¬æ‰©å±•æºï¼‰"""
    if sources is None:
        sources = list(SOURCES.keys())
    
    print(f"\n{'='*60}")
    print(f"çƒ­ç‚¹é‡‡é›†å¼€å§‹ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")
    
    results = {}
    all_items = []
    total_new = 0
    
    # 1. é‡‡é›†åŸºç¡€æ•°æ®æº (RSSHub)
    print("ã€åŸºç¡€æ•°æ®æº - RSSHubã€‘")
    print("-" * 40)
    for source_id in sources:
        if source_id not in SOURCES:
            continue
        
        source = SOURCES[source_id]
        print(f"[{source['name']}] é‡‡é›†ä¸­...")
        
        items = parse_rss(source["url"], source_id)
        
        if items:
            new_count = save_to_daily(items)
            save_to_source(items, source_id)
            all_items.extend(items)
            
            print(f"[{source['name']}] âœ… è·å– {len(items)} æ¡ï¼Œæ–°å¢ {new_count} æ¡")
            results[source_id] = new_count
            total_new += new_count
        else:
            print(f"[{source['name']}] âŒ è·å–å¤±è´¥")
            results[source_id] = 0
    
    # 2. é‡‡é›†æ‰©å±•æ•°æ®æºï¼ˆç›´æ¥RSSï¼‰
    if include_extended:
        print(f"\nã€æ‰©å±•æ•°æ®æº - ç›´æ¥RSSã€‘")
        print("-" * 40)
        
        extended = get_extended_sources()
        for source_id, source_info in extended.items():
            print(f"[{source_info['name']}] é‡‡é›†ä¸­...")
            items = fetch_extended_rss(source_info["url"], source_id, source_info)
            
            if items:
                new_count = save_to_daily(items)
                save_to_source(items, source_id)
                all_items.extend(items)
                
                print(f"[{source_info['name']}] âœ… è·å– {len(items)} æ¡ï¼Œæ–°å¢ {new_count} æ¡")
                results[source_id] = new_count
                total_new += new_count
            else:
                print(f"[{source_info['name']}] âŒ è·å–å¤±è´¥")
                results[source_id] = 0
    
    # æ›´æ–°æ€»ç´¢å¼•
    if all_items:
        update_index(all_items)
    
    print(f"\n{'='*60}")
    print(f"é‡‡é›†å®Œæˆ - å…±è·å– {len(all_items)} æ¡ï¼Œæ–°å¢ {total_new} æ¡çƒ­ç‚¹")
    print(f"{'='*60}\n")
    
    return results


def get_extended_sources() -> Dict:
    """è·å–å¯ç”¨çš„æ‰©å±•æ•°æ®æº"""
    config_file = os.path.join(STORAGE_DIR, "extended_sources.json")
    if os.path.exists(config_file):
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
            return config.get("working_sources", {})
    return {}


def fetch_extended_rss(url: str, source_id: str, source_info: Dict) -> List[Dict]:
    """è·å–æ‰©å±•RSSæº"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "application/rss+xml,application/xml,text/xml,*/*"
        }
        req = Request(url, headers=headers)
        with urlopen(req, timeout=30) as resp:
            content = resp.read().decode('utf-8', errors='ignore')
        
        items = []
        now = datetime.now()
        timestamp = now.strftime("%Y-%m-%d %H:%M:%S")
        date_str = now.strftime("%Y-%m-%d")
        
        # è§£æ RSS
        patterns = [
            r'<item>.*?<title>(?:<!\[CDATA\[)?(.*?)(?:\]\]>)?</title>.*?<link>(?:<!\[CDATA\[)?(.*?)(?:\]\]>)?</link>',
            r'<entry>.*?<title>(?:<!\[CDATA\[)?(.*?)(?:\]\]>)?</title>.*?<link[^>]*href="(.*?)"',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content, re.DOTALL)
            if matches:
                break
        
        for title, link in matches[:30]:
            title = title.strip()
            link = link.strip()
            
            if not title or len(title) < 5:
                continue
            
            item = {
                "id": generate_id(title, source_id, timestamp),
                "title": title,
                "url": link,
                "source_id": source_id,
                "source_name": source_info["name"],
                "platform": source_info["platform"],
                "category": source_info["category"],
                "crawl_time": timestamp,
                "crawl_date": date_str,
                "pub_time": None
            }
            items.append(item)
        
        return items
    except Exception as e:
        return []


def query_by_date(date_str: str) -> List[Dict]:
    """æŒ‰æ—¥æœŸæŸ¥è¯¢"""
    daily_file = os.path.join(STORAGE_DIR, "daily", f"{date_str}.json")
    if os.path.exists(daily_file):
        with open(daily_file, 'r', encoding='utf-8') as f:
            return list(json.load(f).values())
    return []


def query_by_source(source_id: str, limit: int = 50) -> List[Dict]:
    """æŒ‰æ¥æºæŸ¥è¯¢"""
    source_file = os.path.join(STORAGE_DIR, "by_source", f"{source_id}.json")
    if os.path.exists(source_file):
        with open(source_file, 'r', encoding='utf-8') as f:
            items = json.load(f)
            return items[-limit:]
    return []


def query_by_keyword(keyword: str, days: int = 7) -> List[Dict]:
    """æŒ‰å…³é”®è¯æŸ¥è¯¢"""
    results = []
    keyword_lower = keyword.lower()
    
    for i in range(days):
        date = (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
        items = query_by_date(date)
        for item in items:
            if keyword_lower in item["title"].lower():
                results.append(item)
    
    return results


def get_stats() -> Dict:
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    index_file = os.path.join(STORAGE_DIR, "index.json")
    if os.path.exists(index_file):
        with open(index_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}


def cleanup_old_data(days: int = 30) -> Dict:
    """æ¸…ç†æ—§æ•°æ®"""
    cutoff_date = datetime.now() - timedelta(days=days)
    cutoff_str = cutoff_date.strftime("%Y-%m-%d")
    
    daily_dir = os.path.join(STORAGE_DIR, "daily")
    removed_files = []
    removed_count = 0
    
    if os.path.exists(daily_dir):
        for filename in os.listdir(daily_dir):
            if filename.endswith(".json"):
                date_str = filename.replace(".json", "")
                if date_str < cutoff_str:
                    filepath = os.path.join(daily_dir, filename)
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        removed_count += len(data)
                    os.remove(filepath)
                    removed_files.append(date_str)
    
    print(f"âœ… æ¸…ç†å®Œæˆ: åˆ é™¤ {len(removed_files)} å¤©æ•°æ®ï¼Œå…± {removed_count} æ¡çƒ­ç‚¹")
    print(f"   æ¸…ç†æ—¥æœŸ: {', '.join(removed_files[:10])}{'...' if len(removed_files) > 10 else ''}")
    
    return {
        "removed_days": len(removed_files),
        "removed_items": removed_count,
        "dates": removed_files
    }


def generate_daily_report(date_str: str = None) -> Dict:
    """ç”Ÿæˆæ¯æ—¥çƒ­ç‚¹æŠ¥å‘Š"""
    if not date_str:
        date_str = datetime.now().strftime("%Y-%m-%d")
    
    items = query_by_date(date_str)
    
    if not items:
        return {"error": f"æ—  {date_str} æ•°æ®"}
    
    # ç»Ÿè®¡
    sources = {}
    categories = {}
    keywords = {}
    
    for item in items:
        # æ¥æºç»Ÿè®¡
        src = item["source_name"]
        sources[src] = sources.get(src, 0) + 1
        
        # åˆ†ç±»ç»Ÿè®¡
        cat = item["category"]
        categories[cat] = categories.get(cat, 0) + 1
        
        # å…³é”®è¯æå–ï¼ˆç®€å•ç‰ˆï¼šæå–æ ‡é¢˜ä¸­çš„ä¸­æ–‡è¯ï¼‰
        title = item["title"]
        # ç»Ÿè®¡å¸¸è§è¯
        for word in ["AI", "ç§‘æŠ€", "æ•™è‚²", "å¿ƒç†", "å­¦ä¹ ", "æŠ€æœ¯", "å¼€å‘", "æ•°æ®", "ç®—æ³•", "æ¨¡å‹"]:
            if word in title:
                keywords[word] = keywords.get(word, 0) + 1
    
    # çƒ­é—¨æ ‡é¢˜
    sorted_items = sorted(items, key=lambda x: x.get("crawl_time", ""), reverse=True)
    
    report = {
        "date": date_str,
        "total": len(items),
        "sources": dict(sorted(sources.items(), key=lambda x: -x[1])),
        "categories": dict(sorted(categories.items(), key=lambda x: -x[1])),
        "top_keywords": dict(sorted(keywords.items(), key=lambda x: -x[1])[:10]),
        "latest_5": [{"title": i["title"], "source": i["source_name"]} for i in sorted_items[:5]]
    }
    
    # æ‰“å°æŠ¥å‘Š
    print(f"\n{'='*60}")
    print(f"ğŸ“Š çƒ­ç‚¹æ—¥æŠ¥ - {date_str}")
    print(f"{'='*60}")
    print(f"\nğŸ“ˆ æ€»è®¡: {report['total']} æ¡çƒ­ç‚¹")
    
    print(f"\nğŸ“¡ æ¥æºåˆ†å¸ƒ:")
    for src, count in list(report['sources'].items())[:5]:
        print(f"   {src}: {count} æ¡")
    
    print(f"\nğŸ·ï¸ åˆ†ç±»åˆ†å¸ƒ:")
    for cat, count in report['categories'].items():
        print(f"   {cat}: {count} æ¡")
    
    if report['top_keywords']:
        print(f"\nğŸ”‘ çƒ­é—¨å…³é”®è¯:")
        for kw, count in list(report['top_keywords'].items())[:5]:
            print(f"   {kw}: {count} æ¬¡")
    
    print(f"\nğŸ“° æœ€æ–°çƒ­ç‚¹:")
    for i, item in enumerate(report['latest_5'], 1):
        print(f"   {i}. [{item['source']}] {item['title'][:30]}...")
    
    print(f"\n{'='*60}\n")
    
    return report


def retry_failed_sources(failed_sources: List[str], max_retries: int = 3) -> Dict:
    """é‡è¯•å¤±è´¥çš„é‡‡é›†"""
    results = {}
    
    for source_id in failed_sources:
        if source_id not in SOURCES:
            continue
        
        source = SOURCES[source_id]
        for attempt in range(max_retries):
            try:
                print(f"[{source['name']}] é‡è¯• {attempt + 1}/{max_retries}...")
                items = parse_rss(source["url"], source_id)
                if items:
                    save_to_daily(items)
                    save_to_source(items, source_id)
                    results[source_id] = len(items)
                    print(f"[{source['name']}] âœ… é‡è¯•æˆåŠŸ: {len(items)} æ¡")
                    break
            except Exception as e:
                print(f"[{source['name']}] âŒ é‡è¯•å¤±è´¥: {str(e)[:30]}")
                results[source_id] = 0
    
    return results


def main():
    import argparse
    parser = argparse.ArgumentParser(description='çƒ­ç‚¹æ•°æ®å­˜å‚¨ç³»ç»Ÿ')
    parser.add_argument('--crawl', '-c', action='store_true', help='é‡‡é›†æ‰€æœ‰æ•°æ®æº')
    parser.add_argument('--sources', '-s', default='', help='æŒ‡å®šæ•°æ®æºï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--query-date', '-d', help='æŒ‰æ—¥æœŸæŸ¥è¯¢ (YYYY-MM-DD)')
    parser.add_argument('--query-source', help='æŒ‰æ¥æºæŸ¥è¯¢')
    parser.add_argument('--query-keyword', '-k', help='æŒ‰å…³é”®è¯æŸ¥è¯¢')
    parser.add_argument('--stats', action='store_true', help='æ˜¾ç¤ºç»Ÿè®¡')
    parser.add_argument('--report', '-r', action='store_true', help='ç”Ÿæˆæ—¥æŠ¥')
    parser.add_argument('--cleanup', type=int, metavar='DAYS', help='æ¸…ç†DAYSå¤©å‰çš„æ—§æ•°æ®')
    parser.add_argument('--retry', action='store_true', help='é‡è¯•å¤±è´¥çš„é‡‡é›†')
    args = parser.parse_args()
    
    if args.crawl:
        sources = args.sources.split(',') if args.sources else None
        crawl_all_sources(sources)
    elif args.query_date:
        items = query_by_date(args.query_date)
        print(json.dumps(items, ensure_ascii=False, indent=2))
    elif args.query_source:
        items = query_by_source(args.query_source)
        print(json.dumps(items, ensure_ascii=False, indent=2))
    elif args.query_keyword:
        items = query_by_keyword(args.query_keyword)
        print(json.dumps(items, ensure_ascii=False, indent=2))
    elif args.stats:
        stats = get_stats()
        print(json.dumps(stats, ensure_ascii=False, indent=2))
    elif args.report:
        generate_daily_report()
    elif args.cleanup:
        cleanup_old_data(args.cleanup)
    elif args.retry:
        # ä»æ—¥å¿—è¯»å–å¤±è´¥çš„æºï¼ˆç®€åŒ–ç‰ˆï¼šé‡è¯•æ‰€æœ‰æºï¼‰
        crawl_all_sources()
    else:
        # é»˜è®¤ï¼šé‡‡é›†æ‰€æœ‰
        crawl_all_sources()


if __name__ == '__main__':
    main()