#!/usr/bin/env python3
"""
æ‰©å±•æ•°æ®æºé‡‡é›†å™¨ v2
ä¿®å¤ RSSHub JSON æ ¼å¼é—®é¢˜ï¼Œæ·»åŠ æ›´å¤šæ•°æ®æº

ä½œè€…: AI Article Publisher
åˆ›å»ºæ—¶é—´: 2026-02-22
æ›´æ–°æ—¶é—´: 2026-02-23
"""

import json
import time
import hashlib
import urllib.request
import urllib.error
import os
from datetime import datetime
from typing import List, Dict, Any, Optional

# ============================================
# RSSHub æ•°æ®æº (JSON æ ¼å¼)
# ============================================

RSSHUB_BASE = "http://localhost:1200"

RSSHUB_SOURCES = {
    # ä¸­æ–‡çƒ­æ¦œ
    "zhihu_hot": {
        "name": "çŸ¥ä¹çƒ­æ¦œ",
        "route": "/zhihu/hot",
        "category": "ç»¼åˆ",
        "platform": "çŸ¥ä¹"
    },
    "weibo_suggest": {
        "name": "å¾®åšçƒ­è®®",
        "route": "/weibo/suggest/hot",
        "category": "ç»¼åˆ",
        "platform": "å¾®åš"
    },
    "baidu_tieba": {
        "name": "ç™¾åº¦è´´å§çƒ­è®®",
        "route": "/tieba/hot",
        "category": "ç»¼åˆ",
        "platform": "ç™¾åº¦"
    },
    "douban_group": {
        "name": "è±†ç“£å°ç»„",
        "route": "/douban/group/explore",
        "category": "ç»¼åˆ",
        "platform": "è±†ç“£"
    },
    "toutiao": {
        "name": "ä»Šæ—¥å¤´æ¡",
        "route": "/toutiao/hot",
        "category": "ç»¼åˆ",
        "platform": "ä»Šæ—¥å¤´æ¡"
    },
    
    # ç§‘æŠ€èµ„è®¯
    "hackernews_best": {
        "name": "Hacker News",
        "route": "/hackernews/best",
        "category": "ç§‘æŠ€",
        "platform": "Hacker News"
    },
    "hackernews_top": {
        "name": "HN Top",
        "route": "/hackernews/top",
        "category": "ç§‘æŠ€",
        "platform": "Hacker News"
    },
    "github_trending": {
        "name": "GitHub Trending",
        "route": "/github/trending/daily",
        "category": "ç§‘æŠ€",
        "platform": "GitHub"
    },
    "github_trending_python": {
        "name": "GitHub Python",
        "route": "/github/trending/daily/https://github.com/trending/python",
        "category": "ç§‘æŠ€",
        "platform": "GitHub"
    },
    "v2ex_hot": {
        "name": "V2EX",
        "route": "/v2ex/topics/hot",
        "category": "ç§‘æŠ€",
        "platform": "V2EX"
    },
    "juejin_hot": {
        "name": "æ˜é‡‘çƒ­æ¦œ",
        "route": "/juejin/posts/hot",
        "category": "ç§‘æŠ€",
        "platform": "æ˜é‡‘"
    },
    "ithome_ranking": {
        "name": "ITä¹‹å®¶çƒ­æ¦œ",
        "route": "/ithome/ranking/7days",
        "category": "ç§‘æŠ€",
        "platform": "ITä¹‹å®¶"
    },
    "sspai_index": {
        "name": "å°‘æ•°æ´¾",
        "route": "/sspai/index",
        "category": "ç§‘æŠ€",
        "platform": "å°‘æ•°æ´¾"
    },
    "infoq": {
        "name": "InfoQ",
        "route": "/infoq/recommend",
        "category": "ç§‘æŠ€",
        "platform": "InfoQ"
    },
    
    # è´¢ç»
    "36kr_news": {
        "name": "36æ°ªå¿«è®¯",
        "route": "/36kr/newsflashes",
        "category": "è´¢ç»",
        "platform": "36æ°ª"
    },
    "wallstreetcn": {
        "name": "åå°”è¡—è§é—»",
        "route": "/wallstreetcn/news/global",
        "category": "è´¢ç»",
        "platform": "åå°”è¡—è§é—»"
    },
    "caixin": {
        "name": "è´¢æ–°ç½‘",
        "route": "/caixin/weekly",
        "category": "è´¢ç»",
        "platform": "è´¢æ–°"
    },
    
    # å¨±ä¹
    "douban_movie": {
        "name": "è±†ç“£ç”µå½±",
        "route": "/douban/movie/playing",
        "category": "å¨±ä¹",
        "platform": "è±†ç“£"
    },
    
    # å›½é™…
    "producthunt": {
        "name": "ProductHunt",
        "route": "/producthunt/today",
        "category": "ç§‘æŠ€",
        "platform": "ProductHunt"
    },
    "reddit_popular": {
        "name": "Reddit Popular",
        "route": "/reddit/popular",
        "category": "å›½é™…",
        "platform": "Reddit"
    },
    "reddit_programming": {
        "name": "Reddit Programming",
        "route": "/reddit/programming",
        "category": "ç§‘æŠ€",
        "platform": "Reddit"
    },
    
    # è®¾è®¡
    "behance": {
        "name": "Behance",
        "route": "/behance/collections",
        "category": "è®¾è®¡",
        "platform": "Behance"
    },
    "dribbble": {
        "name": "Dribbble",
        "route": "/dribbble/popular",
        "category": "è®¾è®¡",
        "platform": "Dribbble"
    },
    
    # å­¦æœ¯
    "nature": {
        "name": "Nature",
        "route": "/nature/news",
        "category": "å­¦æœ¯",
        "platform": "Nature"
    },
    "science": {
        "name": "Science",
        "route": "/science/news",
        "category": "å­¦æœ¯",
        "platform": "Science"
    },
}


def fetch_rsshub_json(route: str) -> Optional[List[Dict]]:
    """ä» RSSHub è·å–æ•°æ® (æ”¯æŒ RSS å’Œ JSON æ ¼å¼)"""
    # å…ˆå°è¯• JSON æ ¼å¼
    url = f"{RSSHUB_BASE}{route}?format=json"
    
    try:
        req = urllib.request.Request(
            url,
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'application/json'
            }
        )
        with urllib.request.urlopen(req, timeout=30) as response:
            data = json.loads(response.read().decode('utf-8'))
            return parse_rsshub_items(data, route)
    except Exception as e:
        # JSON å¤±è´¥ï¼Œå°è¯• RSS æ ¼å¼
        try:
            rss_url = f"{RSSHUB_BASE}{route}"
            req = urllib.request.Request(
                rss_url,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'application/rss+xml'
                }
            )
            with urllib.request.urlopen(req, timeout=30) as response:
                xml_content = response.read().decode('utf-8')
                return parse_rsshub_rss(xml_content, route)
        except Exception as e2:
            print(f"  âŒ RSSHub é”™è¯¯: {route} - {str(e2)[:50]}")
            return None


def parse_rsshub_rss(xml_content: str, route: str) -> List[Dict]:
    """è§£æ RSSHub è¿”å›çš„ RSS XML æ•°æ®"""
    import xml.etree.ElementTree as ET
    
    items = []
    try:
        root = ET.fromstring(xml_content)
        raw_items = root.findall('.//item')
        
        for item in raw_items:
            try:
                title_elem = item.find('title')
                link_elem = item.find('link')
                
                if title_elem is None:
                    continue
                    
                title = title_elem.text or ''
                
                parsed = {
                    "id": generate_id(title + route),
                    "title": title,
                    "url": link_elem.text if link_elem is not None else '',
                    "source_route": route,
                    "crawl_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    "crawl_date": datetime.now().strftime('%Y-%m-%d'),
                }
                
                # æå–æè¿°
                desc_elem = item.find('description')
                if desc_elem is not None and desc_elem.text:
                    import re
                    desc = re.sub(r'<[^>]+>', '', desc_elem.text)
                    parsed['description'] = desc[:200]
                
                items.append(parsed)
            except Exception:
                continue
    except Exception as e:
        print(f"  RSS è§£æé”™è¯¯: {str(e)[:30]}")
    
    return items


def parse_rsshub_items(data: Dict, route: str) -> List[Dict]:
    """è§£æ RSSHub è¿”å›çš„ JSON æ•°æ®"""
    items = []
    
    # RSSHub JSON æ ¼å¼: {item: [...]}
    raw_items = data.get('item', data.get('items', []))
    
    if not raw_items:
        return items
    
    for item in raw_items:
        try:
            title = item.get('title', '')
            if not title:
                continue
                
            parsed = {
                "id": generate_id(title + route),
                "title": title,
                "url": item.get('link', item.get('url', '')),
                "source_route": route,
                "crawl_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                "crawl_date": datetime.now().strftime('%Y-%m-%d'),
            }
            
            # æå–æè¿°
            desc = item.get('description', item.get('summary', item.get('content', '')))
            if desc:
                # æ¸…ç† HTML æ ‡ç­¾
                import re
                desc = re.sub(r'<[^>]+>', '', str(desc))
                parsed['description'] = desc[:200]
            
            items.append(parsed)
        except Exception:
            continue
    
    return items


def generate_id(text: str) -> str:
    """ç”Ÿæˆå”¯ä¸€ID"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()[:12]


def collect_all_sources() -> Dict[str, Any]:
    """é‡‡é›†æ‰€æœ‰æ•°æ®æº"""
    results = {
        "stats": {
            "total": 0,
            "success": 0,
            "failed": 0,
        },
        "sources": {},
        "items": [],
        "crawl_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    print(f"\n{'='*60}")
    print(f"ğŸ“¡ æ‰©å±•æ•°æ®æºé‡‡é›†")
    print(f"{'='*60}")
    print(f"RSSHub æ•°æ®æº: {len(RSSHUB_SOURCES)} ä¸ª")
    print(f"{'='*60}\n")
    
    total_items = 0
    success_count = 0
    
    for source_id, source_info in RSSHUB_SOURCES.items():
        print(f"[{source_info['category'][:2]}] {source_info['name']}...", end=" ")
        
        items = fetch_rsshub_json(source_info['route'])
        
        if items and len(items) > 0:
            # æ·»åŠ å…ƒæ•°æ®
            for item in items:
                item['source_name'] = source_info['name']
                item['category'] = source_info['category']
                item['platform'] = source_info['platform']
            
            results['sources'][source_id] = {
                "name": source_info['name'],
                "count": len(items),
                "category": source_info['category'],
                "platform": source_info['platform']
            }
            results['items'].extend(items)
            
            total_items += len(items)
            success_count += 1
            print(f"âœ… {len(items)} æ¡")
        else:
            print("âŒ æ— æ•°æ®")
        
        time.sleep(0.3)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    results['stats']['total'] = len(RSSHUB_SOURCES)
    results['stats']['success'] = success_count
    results['stats']['failed'] = len(RSSHUB_SOURCES) - success_count
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š é‡‡é›†å®Œæˆ")
    print(f"æˆåŠŸ: {success_count}/{len(RSSHUB_SOURCES)} ä¸ªæº")
    print(f"æ€»æ•°æ®: {total_items} æ¡")
    print(f"{'='*60}\n")
    
    return results


def save_results(results: Dict, output_dir: str = "data/hotnews"):
    """ä¿å­˜é‡‡é›†ç»“æœ"""
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(f"{output_dir}/daily", exist_ok=True)
    os.makedirs(f"{output_dir}/by_source", exist_ok=True)
    
    today = datetime.now().strftime('%Y-%m-%d')
    
    # ä¿å­˜åˆ°æ¯æ—¥æ–‡ä»¶
    daily_file = f"{output_dir}/daily/{today}_extended.json"
    with open(daily_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“ æ¯æ—¥æ–‡ä»¶: {daily_file}")
    
    # æ›´æ–°ç´¢å¼•
    index_file = f"{output_dir}/index.json"
    index = {}
    if os.path.exists(index_file):
        with open(index_file, 'r', encoding='utf-8') as f:
            index = json.load(f)
    
    # æ›´æ–°ç»Ÿè®¡
    if 'extended' not in index:
        index['extended'] = {}
    
    index['extended'][today] = {
        "total_items": len(results['items']),
        "sources": len(results['sources']),
        "crawl_time": results['crawl_time']
    }
    index['last_update'] = results['crawl_time']
    
    with open(index_file, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“ ç´¢å¼•æ–‡ä»¶: {index_file}")
    
    return daily_file


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*60)
    print("ğŸš€ æ‰©å±•æ•°æ®æºé‡‡é›†å™¨ v2")
    print("="*60)
    
    # é‡‡é›†æ•°æ®
    results = collect_all_sources()
    
    # ä¿å­˜ç»“æœ
    if results['items']:
        save_results(results)
        print(f"\nâœ… é‡‡é›†å®Œæˆ! å…± {len(results['items'])} æ¡çƒ­ç‚¹")
    else:
        print("\nâš ï¸  æœªé‡‡é›†åˆ°æ•°æ®")


if __name__ == '__main__':
    main()