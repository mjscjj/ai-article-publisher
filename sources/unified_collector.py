#!/usr/bin/env python3
"""
ç»Ÿä¸€æ•°æ®é‡‡é›†å…¥å£
æ•´åˆ RSSHub + DailyHotApi æ‰€æœ‰æ•°æ®æº

ä½œè€…: AI Article Publisher
åˆ›å»ºæ—¶é—´: 2026-02-23
"""

import os
import sys
import json
import time
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥é‡‡é›†å™¨
try:
    from dailyhot_collector import collect_all_platforms as collect_dailyhot
    from dailyhot_collector import PLATFORMS as DAILYHOT_PLATFORMS
    DAILYHOT_AVAILABLE = True
except ImportError:
    DAILYHOT_AVAILABLE = False
    print("âš ï¸  DailyHotApi é‡‡é›†å™¨ä¸å¯ç”¨")

try:
    from extended_collectors_v2 import collect_all_sources as collect_rsshub
    from extended_collectors_v2 import RSSHUB_SOURCES
    RSSHUB_AVAILABLE = True
except ImportError:
    RSSHUB_AVAILABLE = False
    print("âš ï¸  RSSHub é‡‡é›†å™¨ä¸å¯ç”¨")


def merge_results(dailyhot_data: Dict, rsshub_data: Dict) -> Dict:
    """åˆå¹¶ä¸¤ä¸ªé‡‡é›†å™¨çš„ç»“æœ"""
    merged = {
        "stats": {
            "total_items": 0,
            "platforms": 0,
            "dailyhot_items": 0,
            "rsshub_items": 0,
        },
        "platforms": {},
        "items": [],
        "crawl_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    }
    
    # åˆå¹¶ DailyHotApi æ•°æ®
    if dailyhot_data and dailyhot_data.get('items'):
        merged['items'].extend(dailyhot_data['items'])
        merged['stats']['dailyhot_items'] = len(dailyhot_data['items'])
        for pid, pinfo in dailyhot_data.get('platforms', {}).items():
            merged['platforms'][f"dailyhot_{pid}"] = pinfo
    
    # åˆå¹¶ RSSHub æ•°æ®
    if rsshub_data and rsshub_data.get('items'):
        merged['items'].extend(rsshub_data['items'])
        merged['stats']['rsshub_items'] = len(rsshub_data['items'])
        for sid, sinfo in rsshub_data.get('sources', {}).items():
            merged['platforms'][f"rsshub_{sid}"] = sinfo
    
    merged['stats']['total_items'] = len(merged['items'])
    merged['stats']['platforms'] = len(merged['platforms'])
    
    return merged


def deduplicate_items(items: List[Dict]) -> List[Dict]:
    """åŸºäºæ ‡é¢˜å»é‡"""
    seen = set()
    unique = []
    for item in items:
        title = item.get('title', '')
        if title and title not in seen:
            seen.add(title)
            unique.append(item)
    return unique


def save_unified_data(data: Dict, output_dir: str = "data/hotnews"):
    """ä¿å­˜ç»Ÿä¸€æ•°æ®"""
    os.makedirs(f"{output_dir}/daily", exist_ok=True)
    
    today = datetime.now().strftime('%Y-%m-%d')
    
    # ä¿å­˜æ¯æ—¥åˆå¹¶æ•°æ®
    daily_file = f"{output_dir}/daily/{today}_unified.json"
    with open(daily_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“ æ¯æ—¥æ–‡ä»¶: {daily_file}")
    
    # æ›´æ–°ç´¢å¼•
    index_file = f"{output_dir}/index.json"
    index = {}
    if os.path.exists(index_file):
        with open(index_file, 'r', encoding='utf-8') as f:
            index = json.load(f)
    
    # æ›´æ–°ç»Ÿè®¡
    index['unified'] = {
        today: {
            "total_items": data['stats']['total_items'],
            "platforms": data['stats']['platforms'],
            "dailyhot_items": data['stats']['dailyhot_items'],
            "rsshub_items": data['stats']['rsshub_items'],
            "crawl_time": data['crawl_time']
        }
    }
    index['last_update'] = data['crawl_time']
    index['total_items'] = index.get('total_items', 0) + data['stats']['total_items']
    
    with open(index_file, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“ ç´¢å¼•æ–‡ä»¶: {index_file}")
    
    return daily_file


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*60)
    print("ğŸš€ ç»Ÿä¸€æ•°æ®é‡‡é›†å™¨")
    print("="*60)
    print(f"DailyHotApi: {'âœ… å¯ç”¨' if DAILYHOT_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")
    print(f"RSSHub: {'âœ… å¯ç”¨' if RSSHUB_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")
    print("="*60 + "\n")
    
    dailyhot_data = {}
    rsshub_data = {}
    
    # é‡‡é›† DailyHotApi
    if DAILYHOT_AVAILABLE:
        print("\nğŸ“¡ é‡‡é›† DailyHotApi æ•°æ®æº...")
        print(f"å¹³å°æ•°é‡: {len(DAILYHOT_PLATFORMS)}")
        dailyhot_data = collect_dailyhot()
    
    # é‡‡é›† RSSHub
    if RSSHUB_AVAILABLE:
        print("\nğŸ“¡ é‡‡é›† RSSHub æ•°æ®æº...")
        print(f"æ•°æ®æºæ•°é‡: {len(RSSHUB_SOURCES)}")
        rsshub_data = collect_rsshub()
    
    # åˆå¹¶æ•°æ®
    print("\nğŸ“Š åˆå¹¶æ•°æ®...")
    merged = merge_results(dailyhot_data, rsshub_data)
    
    # å»é‡
    print(f"å»é‡å‰: {len(merged['items'])} æ¡")
    merged['items'] = deduplicate_items(merged['items'])
    merged['stats']['total_items'] = len(merged['items'])
    print(f"å»é‡å: {len(merged['items'])} æ¡")
    
    # ä¿å­˜
    if merged['items']:
        save_unified_data(merged)
        print(f"\nâœ… é‡‡é›†å®Œæˆ!")
        print(f"æ€»æ•°æ®: {merged['stats']['total_items']} æ¡")
        print(f"å¹³å°æ•°: {merged['stats']['platforms']} ä¸ª")
    else:
        print("\nâš ï¸  æœªé‡‡é›†åˆ°æ•°æ®")


if __name__ == '__main__':
    main()