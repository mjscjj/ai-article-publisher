#!/usr/bin/env python3
"""
ã€æ·±åº¦ä¿¡æ¯æ£€ç´¢ã€‘Deep Information Retriever
å¤šæºæ•°æ®æ£€ç´¢ + æ™ºèƒ½èšåˆ + äº‹å®éªŒè¯

æ•°æ®æº:
1. RAG çŸ¥è¯†åº“ (æœ¬åœ°)
2. çƒ­ç‚¹æ•°æ®æº (sources/)
3. ç½‘ç»œæœç´¢ (enhanced_search.py)
4. å‚ç›´é¢†åŸŸé‡‡é›†å™¨

åŠŸèƒ½:
1. å¤šæºæ£€ç´¢ - å¹¶è¡Œæœç´¢å¤šä¸ªæ•°æ®æº
2. æ™ºèƒ½å»é‡ - åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦
3. å¯ä¿¡åº¦è¯„åˆ† - è¯„ä¼°ä¿¡æ¯å¯é æ€§
4. äº‹å®æå– - ç»“æ„åŒ–æå–å…³é”®ä¿¡æ¯
"""

import os
import sys
import json
import hashlib
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from collections import Counter

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

class DeepRetriever:
    """æ·±åº¦ä¿¡æ¯æ£€ç´¢å™¨"""
    
    def __init__(self):
        # æ•°æ®æºç›®å½•
        self.data_dir = os.path.join(
            os.path.dirname(os.path.dirname(__file__)),
            'data'
        )
        
        # çƒ­ç‚¹æ•°æ®ç¼“å­˜
        self.hot_data_cache = []
        
        # åŠ è½½æœ¬åœ°æ•°æ®
        self._load_local_data()
    
    def _load_local_data(self):
        """åŠ è½½æœ¬åœ°çƒ­ç‚¹æ•°æ®"""
        # åŠ è½½ by_source ç›®å½•ä¸‹çš„æ•°æ®
        by_source_dir = os.path.join(self.data_dir, 'by_source')
        if os.path.exists(by_source_dir):
            for filename in os.listdir(by_source_dir):
                if filename.endswith('.json'):
                    filepath = os.path.join(by_source_dir, filename)
                    try:
                        with open(filepath, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            if isinstance(data, list):
                                self.hot_data_cache.extend(data)
                    except:
                        pass
        
        print(f"[Retriever] âœ… åŠ è½½ {len(self.hot_data_cache)} æ¡æœ¬åœ°æ•°æ®")
    
    def retrieve(self, topic: str, 
                 top_k: int = 10,
                 sources: List[str] = None) -> List[Dict[str, Any]]:
        """
        å¤šæºæ£€ç´¢
        
        Args:
            topic: æ£€ç´¢è¯é¢˜
            top_k: è¿”å›æ•°é‡
            sources: æŒ‡å®šæ•°æ®æº (å¯é€‰)
        
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨ï¼ŒæŒ‰ç›¸å…³æ€§æ’åº
        """
        results = []
        
        # 1. RAG æ£€ç´¢
        rag_results = self._search_rag(topic, top_k // 2)
        results.extend(rag_results)
        
        # 2. æœ¬åœ°çƒ­ç‚¹æ£€ç´¢
        local_results = self._search_local(topic, top_k)
        results.extend(local_results)
        
        # 3. å»é‡
        deduped = self._deduplicate(results)
        
        # 4. å¯ä¿¡åº¦è¯„åˆ†
        scored = self._score_credibility(deduped)
        
        # 5. æ’åº
        scored.sort(key=lambda x: (x.get('relevance', 0), x.get('credibility', 0)), reverse=True)
        
        return scored[:top_k]
    
    def _search_rag(self, topic: str, top_k: int) -> List[Dict]:
        """RAG æ£€ç´¢"""
        try:
            from core.rag_simple import SimpleRAG
            rag = SimpleRAG()
            results = rag.search(topic, top_k=top_k)
            
            # æ ‡å‡†åŒ–æ ¼å¼
            for r in results:
                r['source'] = 'RAG'
                r['source_type'] = 'knowledge_base'
            
            print(f"[Retriever] RAG æ£€ç´¢åˆ° {len(results)} æ¡")
            return results
        except Exception as e:
            print(f"[Retriever] âš ï¸ RAG æ£€ç´¢å¤±è´¥ï¼š{e}")
            return []
    
    def _search_local(self, topic: str, top_k: int) -> List[Dict]:
        """æœ¬åœ°çƒ­ç‚¹æ£€ç´¢"""
        results = []
        topic_keywords = self._extract_keywords(topic)
        
        for item in self.hot_data_cache:
            # è®¡ç®—ç›¸å…³æ€§
            title = item.get('title', '')
            score = self._calculate_relevance(topic_keywords, title)
            
            if score > 0:
                results.append({
                    'title': title,
                    'content': item.get('content', item.get('snippet', '')),
                    'url': item.get('url', ''),
                    'source': item.get('source_name', 'Unknown'),
                    'source_type': 'hot_news',
                    'timestamp': item.get('crawl_time', ''),
                    'relevance': score
                })
        
        print(f"[Retriever] æœ¬åœ°æ£€ç´¢åˆ° {len(results)} æ¡")
        return results
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        import re
        # ä¸­æ–‡ 2-4 å­—è¯ + è‹±æ–‡å•è¯
        keywords = re.findall(r'[\u4e00-\u9fa5]{2,4}|\w+', text.lower())
        return keywords
    
    def _calculate_relevance(self, keywords: List[str], text: str) -> float:
        """è®¡ç®—ç›¸å…³æ€§"""
        text_lower = text.lower()
        matched = sum(1 for kw in keywords if kw in text_lower)
        return matched / max(len(keywords), 1)
    
    def _deduplicate(self, results: List[Dict]) -> List[Dict]:
        """å»é‡ (åŸºäºæ ‡é¢˜ç›¸ä¼¼åº¦)"""
        seen_hashes = set()
        deduped = []
        
        for r in results:
            # è®¡ç®—æ ‡é¢˜å“ˆå¸Œ
            title = r.get('title', '')
            title_hash = hashlib.md5(title.encode('utf-8')).hexdigest()
            
            if title_hash not in seen_hashes:
                seen_hashes.add(title_hash)
                deduped.append(r)
        
        print(f"[Retriever] å»é‡å {len(deduped)} æ¡ (åŸ{len(results)}æ¡)")
        return deduped
    
    def _score_credibility(self, results: List[Dict]) -> List[Dict]:
        """å¯ä¿¡åº¦è¯„åˆ†"""
        source_weights = {
            'RAG': 0.9,
            'å®˜æ–¹åª’ä½“': 0.9,
            'çŸ¥ååª’ä½“': 0.8,
            'è¡Œä¸šåª’ä½“': 0.7,
            'ç¤¾äº¤åª’ä½“': 0.5,
            'Unknown': 0.5
        }
        
        for r in results:
            source = r.get('source', 'Unknown')
            
            # åŸºç¡€å¯ä¿¡åº¦
            credibility = source_weights.get(source, 0.5)
            
            # æœ‰æ—¶æ•ˆæ€§åŠ åˆ†
            timestamp = r.get('timestamp', '')
            if timestamp:
                try:
                    crawl_time = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                    days_old = (datetime.now() - crawl_time).days
                    if days_old <= 1:
                        credibility += 0.1
                    elif days_old <= 3:
                        credibility += 0.05
                except:
                    pass
            
            # æœ‰ URL åŠ åˆ†
            if r.get('url'):
                credibility += 0.05
            
            r['credibility'] = min(1.0, credibility)
        
        return results
    
    def extract_facts(self, results: List[Dict]) -> List[str]:
        """
        ä»æ£€ç´¢ç»“æœä¸­æå–äº‹å®
        
        Returns:
            äº‹å®åˆ—è¡¨
        """
        facts = []
        
        for r in results[:10]:  # å–å‰ 10 æ¡
            # æå–æ ‡é¢˜ä½œä¸ºäº‹å®
            title = r.get('title', '')
            if title and len(title) > 10:
                facts.append(title)
            
            # æå–å†…å®¹æ‘˜è¦
            content = r.get('content', '')
            if content and len(content) > 50:
                # å–ç¬¬ä¸€å¥
                first_sentence = content.split('ã€‚')[0] + 'ã€‚'
                if len(first_sentence) > 20:
                    facts.append(first_sentence)
        
        # å»é‡
        unique_facts = list(dict.fromkeys(facts))
        
        print(f"[Retriever] æå– {len(unique_facts)} æ¡äº‹å®")
        return unique_facts
    
    def get_statistics(self, topic: str, results: List[Dict]) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ£€ç´¢ç»Ÿè®¡
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        source_dist = Counter(r.get('source', 'Unknown') for r in results)
        
        # å¯ä¿¡åº¦åˆ†å¸ƒ
        high_cred = sum(1 for r in results if r.get('credibility', 0) >= 0.8)
        med_cred = sum(1 for r in results if 0.5 <= r.get('credibility', 0) < 0.8)
        low_cred = sum(1 for r in results if r.get('credibility', 0) < 0.5)
        
        return {
            'topic': topic,
            'total_results': len(results),
            'source_distribution': dict(source_dist),
            'credibility': {
                'high': high_cred,
                'medium': med_cred,
                'low': low_cred
            },
            'avg_relevance': sum(r.get('relevance', 0) for r in results) / max(len(results), 1),
            'retrieval_time': datetime.now().isoformat()
        }


def test_deep_retriever():
    """æµ‹è¯•"""
    print("\n" + "="*70)
    print("ğŸ” æ·±åº¦ä¿¡æ¯æ£€ç´¢æµ‹è¯•")
    print("="*70 + "\n")
    
    retriever = DeepRetriever()
    
    # æµ‹è¯•æ£€ç´¢
    topic = "äººå·¥æ™ºèƒ½ æ•™è‚²"
    print(f"æ£€ç´¢è¯é¢˜ï¼š{topic}\n")
    
    results = retriever.retrieve(topic, top_k=10)
    
    print(f"\n{'='*70}")
    print("ğŸ“Š æ£€ç´¢ç»“æœ")
    print(f"{'='*70}\n")
    
    for i, r in enumerate(results[:5], 1):
        print(f"{i}. [{r.get('source', 'Unknown')}] {r.get('title', 'N/A')[:50]}...")
        print(f"   ç›¸å…³æ€§ï¼š{r.get('relevance', 0):.2f} | å¯ä¿¡åº¦ï¼š{r.get('credibility', 0):.2f}\n")
    
    # æå–äº‹å®
    print("="*70)
    print("ğŸ“‹ æå–äº‹å®")
    print("="*70 + "\n")
    
    facts = retriever.extract_facts(results)
    for i, fact in enumerate(facts[:5], 1):
        print(f"{i}. {fact}\n")
    
    # ç»Ÿè®¡
    print("="*70)
    print("ğŸ“ˆ æ£€ç´¢ç»Ÿè®¡")
    print("="*70 + "\n")
    
    stats = retriever.get_statistics(topic, results)
    print(f"æ€»ç»“æœæ•°ï¼š{stats['total_results']}")
    print(f"æ¥æºåˆ†å¸ƒï¼š{stats['source_distribution']}")
    print(f"å¯ä¿¡åº¦åˆ†å¸ƒï¼šé«˜={stats['credibility']['high']}, ä¸­={stats['credibility']['medium']}, ä½={stats['credibility']['low']}")
    print(f"å¹³å‡ç›¸å…³æ€§ï¼š{stats['avg_relevance']:.2f}")
    
    print("\n" + "="*70)
    print("ğŸ‰ æ·±åº¦æ£€ç´¢æµ‹è¯•å®Œæˆ")
    print("="*70 + "\n")


if __name__ == "__main__":
    test_deep_retriever()
