#!/usr/bin/env python3
"""
ã€è¿­ä»£ä¼˜åŒ–å™¨ã€‘Iteration Optimizer
åŸºäºä¸Šä¸€è½®æ–‡ç« è´¨é‡ï¼Œè‡ªåŠ¨ä¼˜åŒ–æœç´¢ç­–ç•¥å’Œå†™ä½œå‚æ•°

åŠŸèƒ½:
1. åˆ†ææ–‡ç« è´¨é‡ (å­—æ•°ã€ç»“æ„ã€å¼•ç”¨å¯†åº¦)
2. è°ƒæ•´æœç´¢å…³é”®è¯ç­–ç•¥
3. ä¼˜åŒ– Prompt ç»„åˆ
4. ç”Ÿæˆæ”¹è¿›å»ºè®®
"""

import json
import os
import re
from typing import Dict, List, Any

class ArticleAnalyzer:
    """æ–‡ç« è´¨é‡åˆ†æå™¨"""
    
    def __init__(self):
        self.ai_cliches = [
            "åœ¨è¿™ä¸ªä¿¡æ¯çˆ†ç‚¸çš„æ—¶ä»£",
            "éšç€ç§‘æŠ€çš„å‘å±•",
            "ä¸å¯å¦è®¤",
            "ç»¼ä¸Šæ‰€è¿°",
            "æ€»è€Œè¨€ä¹‹",
            "åœ¨è¿™ä¸ªå……æ»¡æŒ‘æˆ˜",
            "æˆ‘ä»¬éœ€è¦å…±åŒåŠªåŠ›",
            "è®©æˆ‘ä»¬æºæ‰‹",
            "å±•æœ›æœªæ¥",
            "å…·æœ‰é‡è¦æ„ä¹‰",
        ]
    
    def analyze(self, article_path: str) -> Dict[str, Any]:
        """åˆ†ææ–‡ç« è´¨é‡"""
        with open(article_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # å»é™¤æ€è€ƒé“¾è·¯
        if "ã€ğŸ§  Kimi 2.5 å†…éƒ¨æ¨æ¼”å›è·¯" in content:
            match = re.search(r'ã€ğŸ–‹ï¸ Kimi 2.5 æœ€ç»ˆæ‰§è¡Œå‡ºç¨¿ã€‘\n(.*)', content, re.S)
            content = match.group(1) if match else content
        
        # åŸºæœ¬ç»Ÿè®¡
        char_count = len(content)
        para_count = len([p for p in content.split('\n\n') if p.strip()])
        
        # AI å¥—è¯æ£€æµ‹
        ai_cliche_count = sum(1 for cliche in self.ai_cliches if cliche in content)
        
        # æ•°æ®å¯†åº¦ (æ•°å­—å‡ºç°é¢‘ç‡)
        numbers = re.findall(r'\d+', content)
        data_density = len(numbers) / max(1, char_count / 1000)
        
        # å¼•ç”¨æ£€æµ‹
        quotes = re.findall(r'[""].*?[""]', content)
        quote_density = len(quotes) / max(1, para_count)
        
        # å°æ ‡é¢˜æ£€æµ‹
        headings = re.findall(r'^#{1,3}\s+(.+)$', content, re.M)
        
        # é¡¹ç›®ç¬¦å·æ£€æµ‹ (è¿è§„)
        bullet_violations = len(re.findall(r'^[\-\*]\s+', content, re.M))
        numbered_violations = len(re.findall(r'^\d+\.\s+', content, re.M))
        
        return {
            "char_count": char_count,
            "para_count": para_count,
            "heading_count": len(headings),
            "ai_cliche_count": ai_cliche_count,
            "data_density": round(data_density, 1),
            "quote_density": round(quote_density, 1),
            "bullet_violations": bullet_violations,
            "numbered_violations": numbered_violations,
            "quality_score": self._calculate_score(
                char_count, ai_cliche_count, data_density, 
                quote_density, bullet_violations + numbered_violations
            ),
            "recommendations": self._generate_recommendations(
                char_count, ai_cliche_count, data_density,
                quote_density, bullet_violations + numbered_violations
            ),
        }
    
    def _calculate_score(self, chars, cliches, data_density, quote_density, violations) -> int:
        """è®¡ç®—è´¨é‡åˆ†æ•° (0-100)"""
        score = 50
        
        # å­—æ•°åˆ† (ç›®æ ‡ 1500-3000)
        if 1500 <= chars <= 3000:
            score += 20
        elif 1000 <= chars < 1500 or 3000 < chars <= 4000:
            score += 10
        
        # AI å¥—è¯æ‰£åˆ†
        score -= cliches * 5
        
        # æ•°æ®å¯†åº¦åŠ åˆ†
        if data_density >= 20:
            score += 10
        elif data_density >= 10:
            score += 5
        
        # å¼•ç”¨å¯†åº¦åŠ åˆ†
        if quote_density >= 1.5:
            score += 10
        elif quote_density >= 0.8:
            score += 5
        
        # è¿è§„æ‰£åˆ†
        score -= violations * 10
        
        return max(0, min(100, score))
    
    def _generate_recommendations(self, chars, cliches, data_density, quote_density, violations) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recs = []
        
        if chars < 1500:
            recs.append("ğŸ“ å­—æ•°åå°‘ï¼Œå»ºè®®å¢åŠ æ·±åº¦åˆ†æå’Œæ¡ˆä¾‹")
        elif chars > 4000:
            recs.append("ğŸ“ å­—æ•°è¿‡å¤šï¼Œå»ºè®®ç²¾ç®€å†—ä½™è¡¨è¿°")
        
        if cliches > 0:
            recs.append(f"âš ï¸ å‘ç° {cliches} å¤„ AI å¥—è¯ï¼Œéœ€åˆ é™¤æ›¿æ¢")
        
        if data_density < 10:
            recs.append("ğŸ“Š æ•°æ®å¯†åº¦ä¸è¶³ï¼Œå¢åŠ å…·ä½“æ•°å­—å’ŒæŠ¥å‘Šå¼•ç”¨")
        
        if quote_density < 0.8:
            recs.append("ğŸ’¬ å¼•ç”¨å¯†åº¦ä¸è¶³ï¼Œå¢åŠ å½“äº‹äººåŸè¯å’Œä¸“å®¶è§‚ç‚¹")
        
        if violations > 0:
            recs.append(f"âŒ å‘ç° {violations} å¤„é¡¹ç›®ç¬¦å·è¿è§„ï¼Œæ”¹ç”¨å®Œæ•´æ®µè½")
        
        return recs if recs else ["âœ… æ–‡ç« è´¨é‡è‰¯å¥½ï¼Œæ— éœ€é‡å¤§è°ƒæ•´"]


class IterationOptimizer:
    """è¿­ä»£ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.analyzer = ArticleAnalyzer()
    
    def optimize(self, article_path: str, config_path: str = None) -> Dict[str, Any]:
        """
        åŸºäºæ–‡ç« åˆ†æç»“æœï¼Œä¼˜åŒ–ä¸‹ä¸€è½®é…ç½®
        """
        # åˆ†ææ–‡ç« 
        analysis = self.analyzer.analyze(article_path)
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        optimizations = {
            "prompt_adjustments": [],
            "search_adjustments": [],
            "config_changes": {},
        }
        
        # æ ¹æ®é—®é¢˜è°ƒæ•´ç­–ç•¥
        if analysis["ai_cliche_count"] > 0:
            optimizations["prompt_adjustments"].append(
                "å¼ºåŒ– anti_ai_formatting çº¦æŸï¼Œæ·»åŠ æ›´å¤šç¦æ­¢å¥—è¯ç¤ºä¾‹"
            )
        
        if analysis["data_density"] < 10:
            optimizations["prompt_adjustments"].append(
                "åœ¨ Prompt ä¸­å¼ºåˆ¶è¦æ±‚æ¯ä¸ªè®ºç‚¹é… 1-2 ä¸ªå…·ä½“æ•°æ®"
            )
            optimizations["search_adjustments"].append(
                "æœç´¢æ—¶ä¼˜å…ˆæŠ“å–å«æ•°æ®çš„æ–°é—»æº (è´¢æŠ¥ã€æŠ¥å‘Šã€ç»Ÿè®¡æ•°æ®)"
            )
        
        if analysis["quote_density"] < 0.8:
            optimizations["prompt_adjustments"].append(
                "å¯ç”¨ quote_heavy é£æ ¼ï¼Œå¼ºåˆ¶è¦æ±‚ 3+ å¤„ç›´æ¥å¼•è¯­"
            )
            optimizations["search_adjustments"].append(
                "å¢åŠ çŸ¥ä¹/å¾®åšç­‰å«ç”¨æˆ·è¯„è®ºçš„æ•°æ®æºæƒé‡"
            )
        
        if analysis["char_count"] < 1500:
            optimizations["prompt_adjustments"].append(
                "è°ƒæ•´å­—æ•°è¦æ±‚ä¸º 2000-2500 å­—"
            )
        
        return {
            "analysis": analysis,
            "optimizations": optimizations,
        }


if __name__ == "__main__":
    import sys
    
    article_path = sys.argv[1] if len(sys.argv) > 1 else "data/e2e_test_article.md"
    
    if not os.path.exists(article_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼š{article_path}")
        sys.exit(1)
    
    optimizer = IterationOptimizer()
    result = optimizer.optimize(article_path)
    
    print("\n" + "="*70)
    print("ğŸ“Š æ–‡ç« è´¨é‡åˆ†ææŠ¥å‘Š")
    print("="*70)
    
    analysis = result["analysis"]
    print(f"\nåŸºæœ¬ç»Ÿè®¡:")
    print(f"  å­—æ•°ï¼š{analysis['char_count']}")
    print(f"  æ®µè½ï¼š{analysis['para_count']}")
    print(f"  å°æ ‡é¢˜ï¼š{analysis['heading_count']}")
    
    print(f"\nè´¨é‡æŒ‡æ ‡:")
    print(f"  AI å¥—è¯ï¼š{analysis['ai_cliche_count']} å¤„")
    print(f"  æ•°æ®å¯†åº¦ï¼š{analysis['data_density']} ä¸ª/åƒå­—")
    print(f"  å¼•ç”¨å¯†åº¦ï¼š{analysis['quote_density']} ä¸ª/æ®µ")
    print(f"  æ ¼å¼è¿è§„ï¼š{analysis['bullet_violations'] + analysis['numbered_violations']} å¤„")
    
    print(f"\nç»¼åˆè¯„åˆ†ï¼š{analysis['quality_score']}/100")
    
    print(f"\næ”¹è¿›å»ºè®®:")
    for rec in analysis["recommendations"]:
        print(f"  {rec}")
    
    print("\n" + "="*70)
    print("ğŸ”§ ä¼˜åŒ–å»ºè®®")
    print("="*70)
    
    opts = result["optimizations"]
    if opts["prompt_adjustments"]:
        print("\nPrompt è°ƒæ•´:")
        for adj in opts["prompt_adjustments"]:
            print(f"  - {adj}")
    
    if opts["search_adjustments"]:
        print("\næœç´¢ç­–ç•¥è°ƒæ•´:")
        for adj in opts["search_adjustments"]:
            print(f"  - {adj}")
    
    print("\n")
